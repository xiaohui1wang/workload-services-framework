From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: Huang Guanxu <guanxu.huang@intel.com>
Date: Wed, 1 Mar 2023 18:51:22 +0100

diff --git a/src/svm/svm_fifo.c b/src/svm/svm_fifo.c
index 49b3d1728..ec3b37654 100644
--- a/src/svm/svm_fifo.c
+++ b/src/svm/svm_fifo.c
@@ -954,6 +954,8 @@ svm_fifo_enqueue_nocopy (svm_fifo_t * f, u32 len)
 
   if (rb_tree_is_init (&f->ooo_enq_lookup))
     {
+      if (PREDICT_FALSE (f->ooos_list_head != OOO_SEGMENT_INVALID_INDEX))
+	len += ooo_segment_try_collect (f, len, &tail);
       f->shr->tail_chunk =
 	f_csptr (f, f_lookup_clear_enq_chunks (f, f_tail_cptr (f), tail));
       f->ooo_enq = 0;
@@ -1280,6 +1282,52 @@ svm_fifo_provision_chunks (svm_fifo_t *f, svm_fifo_seg_t *fs, u32 n_segs,
   return fs_index;
 }
 
+int
+svm_fifo_provision_chunks_with_offset (svm_fifo_t *f, svm_fifo_seg_t *fs,
+				       u32 n_segs, u32 len, u32 offset)
+{
+  u32 head, tail, n_avail, head_pos, n_bytes, fs_index = 1, clen;
+  svm_fifo_chunk_t *c;
+
+  f_load_head_tail_prod (f, &head, &tail);
+  ASSERT (len + offset <= svm_fifo_max_enqueue_prod (f));
+  /* load-relaxed: producer owned index */
+
+  if (f_free_count (f, head, tail) < len + offset)
+    return SVM_FIFO_EFULL;
+
+  n_avail = f_chunk_end (f_end_cptr (f)) - tail;
+
+  if (n_avail < len + offset &&
+      f_try_chunk_alloc (f, head, tail, offset + len))
+    return SVM_FIFO_EGROW;
+
+  if (!fs || !n_segs)
+    return 0;
+
+  ooo_segment_add (f, offset, head, tail, len);
+  if (!f->ooo_enq || !f_chunk_includes_pos (f->ooo_enq, tail + offset))
+    f_update_ooo_enq (f, tail + offset, tail + offset + len);
+
+  c = f_tail_cptr (f);
+  head_pos = (tail - offset - c->start_byte);
+  fs[0].data = c->data + head_pos;
+  fs[0].len = clib_min (c->length - head_pos, len);
+  n_bytes = fs[0].len;
+
+  while (n_bytes < len && fs_index < n_segs)
+    {
+      c = f_cptr (f, c->next);
+      clen = clib_min (c->length, len - n_bytes);
+      fs[fs_index].data = c->data + offset;
+      fs[fs_index].len = clen;
+      n_bytes += clen;
+      fs_index += 1;
+    }
+  f->ooo_enq = c;
+  return fs_index;
+}
+
 int
 svm_fifo_segments (svm_fifo_t *f, u32 offset, svm_fifo_seg_t *fs, u32 *n_segs,
 		   u32 max_bytes)
diff --git a/src/svm/svm_fifo.h b/src/svm/svm_fifo.h
index c4cc0382a..bd5b88c1b 100644
--- a/src/svm/svm_fifo.h
+++ b/src/svm/svm_fifo.h
@@ -264,6 +264,8 @@ int svm_fifo_fill_chunk_list (svm_fifo_t * f);
  */
 int svm_fifo_provision_chunks (svm_fifo_t *f, svm_fifo_seg_t *fs, u32 n_segs,
 			       u32 len);
+int svm_fifo_provision_chunks_with_offset (svm_fifo_t *f, svm_fifo_seg_t *fs,
+					   u32 n_segs, u32 len, u32 offset);
 /**
  * Initialize rbtrees used for ooo lookups
  *
diff --git a/src/vnet/session/session.c b/src/vnet/session/session.c
index eaba80f24..4260623ad 100644
--- a/src/vnet/session/session.c
+++ b/src/vnet/session/session.c
@@ -439,6 +439,67 @@ session_enqueue_discard_chain_bytes (vlib_main_t * vm, vlib_buffer_t * b,
     b->total_length_not_including_first_buffer -= n_bytes_to_drop;
 }
 
+always_inline int
+session_rx_dsa_copy (session_worker_t *wrk, svm_fifo_t *f, session_t *s,
+		     vlib_buffer_t *b, u32 offset)
+{
+  u32 n_segs = 2;
+  svm_fifo_seg_t data_fs[n_segs];
+  int n_fs, enqueued = 0;
+  u16 n_transfers = 0;
+  vlib_main_t *vm = wrk->vm;
+
+  u8 *data = vlib_buffer_get_current (b);
+
+  u32 max_enq = svm_fifo_max_enqueue_prod (f);
+  if (!max_enq)
+    return 0;
+  if (!offset)
+    n_fs = svm_fifo_provision_chunks (f, data_fs, n_segs, b->current_length);
+  else
+    n_fs = svm_fifo_provision_chunks_with_offset (f, data_fs, n_segs,
+						  b->current_length, offset);
+  if (n_fs < 0)
+    return 0;
+
+  while (n_transfers < n_fs)
+    {
+      wrk->rx_batch_num++;
+      vlib_dma_batch_add (vm, wrk->rx_batch, data_fs[n_transfers].data, data,
+			  data_fs[n_transfers].len);
+      data += data_fs[n_transfers].len;
+      enqueued += data_fs[n_transfers].len;
+      n_transfers++;
+    }
+  if (!offset)
+    svm_fifo_enqueue_nocopy (f, enqueued);
+  return enqueued;
+}
+
+always_inline int
+session_rx_copy_data (session_worker_t *wrk, svm_fifo_t *f, session_t *s,
+		      vlib_buffer_t *b, u32 offset)
+{
+  int enqueued = 0;
+  if (!offset)
+    {
+      if (PREDICT_TRUE (!wrk->rx_dma_enabled))
+	enqueued =
+	  svm_fifo_enqueue (f, b->current_length, vlib_buffer_get_current (b));
+      else
+	enqueued = session_rx_dsa_copy (wrk, f, s, b, 0);
+    }
+  else
+    {
+      if (PREDICT_TRUE (!wrk->rx_dma_enabled))
+	enqueued = svm_fifo_enqueue_with_offset (f, offset, b->current_length,
+						 vlib_buffer_get_current (b));
+      else
+	enqueued = session_rx_dsa_copy (wrk, f, s, b, offset);
+    }
+  return enqueued;
+}
+
 /**
  * Enqueue buffer chain tail
  */
@@ -449,9 +510,11 @@ session_enqueue_chain_tail (session_t * s, vlib_buffer_t * b,
   vlib_buffer_t *chain_b;
   u32 chain_bi, len, diff;
   vlib_main_t *vm = vlib_get_main ();
-  u8 *data;
+  //u8 *data;
   u32 written = 0;
   int rv = 0;
+  session_worker_t *wrk;
+  wrk = session_main_get_worker (s->thread_index);
 
   if (is_in_order && offset)
     {
@@ -468,13 +531,13 @@ session_enqueue_chain_tail (session_t * s, vlib_buffer_t * b,
   do
     {
       chain_b = vlib_get_buffer (vm, chain_bi);
-      data = vlib_buffer_get_current (chain_b);
+      //data = vlib_buffer_get_current (chain_b);
       len = chain_b->current_length;
       if (!len)
 	continue;
       if (is_in_order)
 	{
-	  rv = svm_fifo_enqueue (s->rx_fifo, len, data);
+          rv = session_rx_copy_data (wrk, s->rx_fifo, s, chain_b, 0);
 	  if (rv == len)
 	    {
 	      written += rv;
@@ -497,7 +560,7 @@ session_enqueue_chain_tail (session_t * s, vlib_buffer_t * b,
 	}
       else
 	{
-	  rv = svm_fifo_enqueue_with_offset (s->rx_fifo, offset, len, data);
+          rv = session_rx_copy_data (wrk, s->rx_fifo, s, chain_b, offset);
 	  if (rv)
 	    {
 	      clib_warning ("failed to enqueue multi-buffer seg");
@@ -556,12 +619,10 @@ session_enqueue_stream_connection (transport_connection_t * tc,
   int enqueued = 0, rv, in_order_off;
 
   s = session_get (tc->s_index, tc->thread_index);
-
   if (is_in_order)
     {
-      enqueued = svm_fifo_enqueue (s->rx_fifo,
-				   b->current_length,
-				   vlib_buffer_get_current (b));
+      session_worker_t *wrk = session_main_get_worker (s->thread_index);
+      enqueued = session_rx_copy_data (wrk, s->rx_fifo, s, b, 0);
       if (PREDICT_FALSE ((b->flags & VLIB_BUFFER_NEXT_PRESENT)
 			 && enqueued >= 0))
 	{
@@ -573,9 +634,8 @@ session_enqueue_stream_connection (transport_connection_t * tc,
     }
   else
     {
-      rv = svm_fifo_enqueue_with_offset (s->rx_fifo, offset,
-					 b->current_length,
-					 vlib_buffer_get_current (b));
+      session_worker_t *wrk = session_main_get_worker (s->thread_index);
+      rv = session_rx_copy_data (wrk, s->rx_fifo, s, b, offset);
       if (PREDICT_FALSE ((b->flags & VLIB_BUFFER_NEXT_PRESENT) && !rv))
 	session_enqueue_chain_tail (s, b, offset + b->current_length, 0);
       /* if something was enqueued, report even this as success for ooo
@@ -2054,7 +2114,7 @@ session_node_enable_dma (u8 is_en, int n_vlibs)
       if (is_en)
 	{
 	  if (config_index >= 0)
-	    wrk->dma_enabled = true;
+	    wrk->dma_enabled = 0;
 	  wrk->dma_trans = (session_dma_transfer *) clib_mem_alloc (
 	    sizeof (session_dma_transfer) * DMA_TRANS_SIZE);
 	  bzero (wrk->dma_trans,
@@ -2071,6 +2131,80 @@ session_node_enable_dma (u8 is_en, int n_vlibs)
     }
 }
 
+void
+session_rx_dma_completion_cb (vlib_main_t *vm, struct vlib_dma_batch *batch)
+{
+  session_worker_t *wrk;
+  wrk = session_main_get_worker (vm->thread_index);
+  session_rx_dma_info *rx_dma_info;
+  rx_dma_info = &wrk->rx_dma_info[wrk->rx_trans_head];
+  vlib_buffer_free (vm, rx_dma_info->from, rx_dma_info->n_vectors);
+  wrk->rx_trans_head++;
+  if (wrk->rx_trans_head == wrk->rx_trans_size)
+    wrk->rx_trans_head = 0;
+  session_main_flush_enqueue_events (TRANSPORT_PROTO_TCP, vm->thread_index);
+  return;
+}
+
+static void
+session_rx_prepare_dma_args (vlib_dma_config_t *args)
+{
+  args->max_transfers = DMA_TRANS_SIZE;
+  args->max_transfer_size = 65536;
+  args->features = 0;
+  args->sw_fallback = 1;
+  args->barrier_before_last = 1;
+  args->callback_fn = session_rx_dma_completion_cb;
+}
+
+static void
+session_node_rx_enable_dma (u8 is_en, int n_vlibs)
+{
+  vlib_dma_config_t rx_args;
+  session_rx_prepare_dma_args (&rx_args);
+  session_worker_t *wrk;
+  vlib_main_t *vm;
+
+  int config_index = -1;
+
+  if (is_en)
+    {
+      vm = vlib_get_main_by_index (0);
+      config_index = vlib_dma_config_add (vm, &rx_args);
+    }
+  else
+    {
+      vm = vlib_get_main_by_index (0);
+      wrk = session_main_get_worker (0);
+      if (wrk->rx_config_index >= 0)
+	vlib_dma_config_del (vm, wrk->rx_config_index);
+    }
+  int i;
+  for (i = 0; i < n_vlibs; i++)
+    {
+      vm = vlib_get_main_by_index (i);
+      wrk = session_main_get_worker (vm->thread_index);
+      wrk->rx_config_index = config_index;
+      if (is_en)
+	{
+	  if (config_index >= 0)
+	    wrk->rx_dma_enabled = 1;
+	  wrk->rx_dma_info = (session_rx_dma_info *) clib_mem_alloc (
+	    sizeof (session_rx_dma_info) * DMA_TRANS_SIZE);
+	  bzero (wrk->rx_dma_info,
+		 sizeof (session_rx_dma_info) * DMA_TRANS_SIZE);
+	}
+      else
+	{
+	  if (wrk->rx_dma_info)
+	    clib_mem_free (wrk->rx_dma_info);
+	}
+      wrk->rx_trans_head = 0;
+      wrk->rx_trans_tail = 0;
+      wrk->rx_trans_size = DMA_TRANS_SIZE;
+    }
+}
+
 void
 session_node_enable_disable (u8 is_en)
 {
@@ -2113,7 +2247,10 @@ session_node_enable_disable (u8 is_en)
     application_enable_rx_mqs_nodes (is_en);
 
   if (sm->dma_enabled)
-    session_node_enable_dma (is_en, n_vlibs);
+    {
+      session_node_enable_dma (is_en, n_vlibs);
+      session_node_rx_enable_dma (is_en, n_vlibs);
+    }
 }
 
 clib_error_t *
diff --git a/src/vnet/session/session.h b/src/vnet/session/session.h
index ab92295ea..279da122f 100644
--- a/src/vnet/session/session.h
+++ b/src/vnet/session/session.h
@@ -93,6 +93,12 @@ typedef struct
   u16 *pending_tx_nexts;
 } session_dma_transfer;
 
+typedef struct
+{
+  u32 from[256];
+  u32 n_vectors;
+} session_rx_dma_info;
+
 typedef struct session_worker_
 {
   CLIB_CACHE_LINE_ALIGN_MARK (cacheline0);
@@ -169,6 +175,15 @@ typedef struct session_worker_
   u16 batch_num;
   vlib_dma_batch_t *batch;
 
+  int rx_config_index;
+  u8 rx_dma_enabled;
+  session_rx_dma_info *rx_dma_info;
+  u16 rx_trans_head;
+  u16 rx_trans_tail;
+  u16 rx_trans_size;
+  u16 rx_batch_num;
+  vlib_dma_batch_t *rx_batch;
+
 #if SESSION_DEBUG
   /** last event poll time by thread */
   clib_time_type_t last_event_poll;
diff --git a/src/vnet/tcp/tcp_input.c b/src/vnet/tcp/tcp_input.c
index a6d135812..5ffc4b9f1 100644
--- a/src/vnet/tcp/tcp_input.c
+++ b/src/vnet/tcp/tcp_input.c
@@ -19,6 +19,7 @@
 #include <vnet/tcp/tcp.h>
 #include <vnet/tcp/tcp_inlines.h>
 #include <vnet/session/session.h>
+#include <vlib/dma/dma.h>
 #include <math.h>
 
 static vlib_error_desc_t tcp_input_error_counters[] = {
@@ -1484,7 +1485,21 @@ tcp46_established_inline (vlib_main_t * vm, vlib_node_runtime_t * node,
 
   vlib_get_buffers (vm, from, bufs, n_left_from);
   b = bufs;
+  session_worker_t *swrk = session_main_get_worker (vm->thread_index);
+  if (PREDICT_FALSE (swrk->rx_dma_enabled))
+    {
+      if (swrk->rx_trans_head ==
+	  ((swrk->rx_trans_tail + 1) & (swrk->rx_trans_size - 1)))
+	return 0;
+      swrk->rx_batch = vlib_dma_batch_new (vm, swrk->rx_config_index);
 
+      session_rx_dma_info *rx_dma_info =
+	&swrk->rx_dma_info[swrk->rx_trans_tail];
+      clib_memcpy_fast (rx_dma_info->from, from,
+			n_left_from * sizeof (from[0]));
+      rx_dma_info->n_vectors = frame->n_vectors;
+    }
+  u8 need_free = 1;
   while (n_left_from > 0)
     {
       u32 error = TCP_ERROR_ACK_OK;
@@ -1538,13 +1553,28 @@ tcp46_established_inline (vlib_main_t * vm, vlib_node_runtime_t * node,
       b += 1;
     }
 
-  errors = session_main_flush_enqueue_events (TRANSPORT_PROTO_TCP,
-					      thread_index);
+  if (PREDICT_FALSE (swrk->rx_dma_enabled))
+    {
+      if (swrk->rx_batch_num)
+	{
+	  vlib_dma_batch_set_cookie (vm, swrk->rx_batch, swrk->rx_trans_tail);
+	  swrk->rx_batch_num = 0;
+	  swrk->rx_trans_tail++;
+	  if (swrk->rx_trans_tail == swrk->rx_trans_size)
+	    swrk->rx_trans_tail = 0;
+	  need_free = 0;
+	}
+      vlib_dma_batch_submit (vm, swrk->rx_batch);
+    }
+  else
+    errors =
+      session_main_flush_enqueue_events (TRANSPORT_PROTO_TCP, thread_index);
   err_counters[TCP_ERROR_MSG_QUEUE_FULL] = errors;
   tcp_store_err_counters (established, err_counters);
   tcp_handle_postponed_dequeues (wrk);
   tcp_handle_disconnects (wrk);
-  vlib_buffer_free (vm, from, frame->n_vectors);
+  if (need_free)
+    vlib_buffer_free (vm, from, frame->n_vectors);
 
   return frame->n_vectors;
 }
@@ -1812,6 +1842,22 @@ tcp46_syn_sent_inline (vlib_main_t *vm, vlib_node_runtime_t *node,
   vlib_get_buffers (vm, from, bufs, n_left_from);
   b = bufs;
 
+  u8 need_free = 1;
+
+  session_worker_t *swrk = session_main_get_worker (vm->thread_index);
+  if (PREDICT_FALSE (swrk->rx_dma_enabled))
+    {
+      if (swrk->rx_trans_head ==
+	  ((swrk->rx_trans_tail + 1) & (swrk->rx_trans_size - 1)))
+	return 0;
+      swrk->rx_batch = vlib_dma_batch_new (vm, swrk->rx_config_index);
+
+      session_rx_dma_info *rx_dma_info =
+	&swrk->rx_dma_info[swrk->rx_trans_tail];
+      clib_memcpy_fast (rx_dma_info->from, from,
+			n_left_from * sizeof (from[0]));
+      rx_dma_info->n_vectors = frame->n_vectors;
+    }
   while (n_left_from > 0)
     {
       u32 ack, seq, error = TCP_ERROR_NONE;
@@ -2034,12 +2080,27 @@ tcp46_syn_sent_inline (vlib_main_t *vm, vlib_node_runtime_t *node,
       tcp_inc_counter (syn_sent, error, 1);
     }
 
-  errors =
-    session_main_flush_enqueue_events (TRANSPORT_PROTO_TCP, thread_index);
+  if (PREDICT_FALSE (swrk->rx_dma_enabled))
+    {
+      if (swrk->rx_batch_num)
+	{
+	  vlib_dma_batch_set_cookie (vm, swrk->rx_batch, swrk->rx_trans_tail);
+	  swrk->rx_batch_num = 0;
+	  swrk->rx_trans_tail++;
+	  if (swrk->rx_trans_tail == swrk->rx_trans_size)
+	    swrk->rx_trans_tail = 0;
+	  need_free = 0;
+	}
+      vlib_dma_batch_submit (vm, swrk->rx_batch);
+    }
+  else
+    errors =
+      session_main_flush_enqueue_events (TRANSPORT_PROTO_TCP, thread_index);
+
   tcp_inc_counter (syn_sent, TCP_ERROR_MSG_QUEUE_FULL, errors);
-  vlib_buffer_free (vm, from, frame->n_vectors);
+  if (need_free)
+    vlib_buffer_free (vm, from, frame->n_vectors);
   tcp_handle_disconnects (wrk);
-
   return frame->n_vectors;
 }
 
@@ -2125,7 +2186,7 @@ always_inline uword
 tcp46_rcv_process_inline (vlib_main_t *vm, vlib_node_runtime_t *node,
 			  vlib_frame_t *frame, int is_ip4)
 {
-  u32 thread_index = vm->thread_index, errors, n_left_from, *from, max_deq;
+  u32 thread_index = vm->thread_index, errors = 0, n_left_from, *from, max_deq;
   tcp_worker_ctx_t *wrk = tcp_get_worker (thread_index);
   vlib_buffer_t *bufs[VLIB_FRAME_SIZE], **b;
 
@@ -2137,7 +2198,21 @@ tcp46_rcv_process_inline (vlib_main_t *vm, vlib_node_runtime_t *node,
 
   vlib_get_buffers (vm, from, bufs, n_left_from);
   b = bufs;
+  u8 need_free = 1;
+  session_worker_t *swrk = session_main_get_worker (vm->thread_index);
+  if (PREDICT_FALSE (swrk->rx_dma_enabled))
+    {
+      if (swrk->rx_trans_head ==
+	  ((swrk->rx_trans_tail + 1) & (swrk->rx_trans_size - 1)))
+	return 0;
+      swrk->rx_batch = vlib_dma_batch_new (vm, swrk->rx_config_index);
 
+      session_rx_dma_info *rx_dma_info =
+	&swrk->rx_dma_info[swrk->rx_trans_tail];
+      clib_memcpy_fast (rx_dma_info->from, from,
+			n_left_from * sizeof (from[0]));
+      rx_dma_info->n_vectors = frame->n_vectors;
+    }
   while (n_left_from > 0)
     {
       u32 error = TCP_ERROR_NONE;
@@ -2395,7 +2470,6 @@ tcp46_rcv_process_inline (vlib_main_t *vm, vlib_node_runtime_t *node,
 	}
 
       /* 6: check the URG bit TODO */
-
       /* 7: process the segment text */
       switch (tc->state)
 	{
@@ -2497,14 +2571,28 @@ tcp46_rcv_process_inline (vlib_main_t *vm, vlib_node_runtime_t *node,
       n_left_from -= 1;
       tcp_inc_counter (rcv_process, error, 1);
     }
-
-  errors = session_main_flush_enqueue_events (TRANSPORT_PROTO_TCP,
-					      thread_index);
+  if (PREDICT_FALSE (swrk->rx_dma_enabled))
+    {
+      if (swrk->rx_batch_num)
+	{
+	  // clib_warning("rcv rx_batch_num %d", swrk->rx_batch_num);
+	  vlib_dma_batch_set_cookie (vm, swrk->rx_batch, swrk->rx_trans_tail);
+	  swrk->rx_batch_num = 0;
+	  swrk->rx_trans_tail++;
+	  if (swrk->rx_trans_tail == swrk->rx_trans_size)
+	    swrk->rx_trans_tail = 0;
+	  need_free = 0;
+	}
+      vlib_dma_batch_submit (vm, swrk->rx_batch);
+    }
+  else
+    errors =
+      session_main_flush_enqueue_events (TRANSPORT_PROTO_TCP, thread_index);
   tcp_inc_counter (rcv_process, TCP_ERROR_MSG_QUEUE_FULL, errors);
   tcp_handle_postponed_dequeues (wrk);
   tcp_handle_disconnects (wrk);
-  vlib_buffer_free (vm, from, frame->n_vectors);
-
+  if (need_free)
+    vlib_buffer_free (vm, from, frame->n_vectors);
   return frame->n_vectors;
 }
 
@@ -2606,7 +2694,6 @@ syn_during_timewait (tcp_connection_t *tc, vlib_buffer_t *b, u32 *iss)
 {
   int paws_reject = tcp_segment_check_paws (tc);
   u32 tw_iss;
-
   *iss = 0;
   /* Check that the SYN arrived out of window. We accept it */
   if (!paws_reject &&
